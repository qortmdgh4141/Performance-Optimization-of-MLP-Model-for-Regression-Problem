{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMqoQgVeN2nBqcSYAO07qbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 패키지 설정**"
      ],
      "metadata": {
        "id": "g4PrYBjjfO7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "from keras import initializers\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout,BatchNormalization\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "ul8pOMMY-rqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 데이터 준비**"
      ],
      "metadata": {
        "id": "DfJ8BCbYfvfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 당뇨병 데이터 셋트 로딩 : 입력 데이터(data), 목표 데이터(target)\n",
        "diabetes = load_diabetes()\n",
        "\n",
        "# 입력 데이터와 목표 데이터를 각각 데이터 프레임으로 변환\n",
        "x_data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
        "y_data = pd.DataFrame(diabetes.target, columns=['target'])\n",
        "\n",
        "# 당뇨병 데이터 셋트에 NaN값이 존재하는지 확인\n",
        "if x_data.isnull().values.any() or y_data.isnull().values.any():\n",
        "    print(\"- 당뇨병 데이터 셋트에는 NaN값이 존재합니다. -\", end= \"\\n\\n\")\n",
        "else:\n",
        "    print(\"- 당뇨병 데이터 셋트에는 NaN값이 존재하지 않습니다. -\", end= \"\\n\\n\")"
      ],
      "metadata": {
        "id": "tvMd0D0SfIJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터를 출력\n",
        "print(f\"< 입력 데이터의 구성 : {x_data.shape[0]}행 x {x_data.shape[1]}열 >\")\n",
        "display(x_data)"
      ],
      "metadata": {
        "id": "P0aB0HX7vRQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분석 대상에서 제외할 변수인 \"age & sex\" 열을 삭제  \n",
        "x_data = x_data.drop(['age', 'sex'], axis=1) \n",
        "\n",
        "#  분석 대상에서 제외할 변수인 \"age & sex\" 열을 삭제한 입력 데이터를 출력\n",
        "print(f\"\\n< 'age & sex' 열을 삭제한 입력 데이터의 구성 : {x_data.shape[0]}행 x {x_data.shape[1]}열 >\")\n",
        "display(x_data)"
      ],
      "metadata": {
        "id": "JbiSuQnM77_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 목표 데이터를 출력\n",
        "print(f\"\\n< 목표 데이터의 구성 : {y_data.shape[0]}행 x {y_data.shape[1]}열 >\")\n",
        "display(y_data)"
      ],
      "metadata": {
        "id": "VbGWIL1U7_ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. 탐색적 데이터 분석**"
      ],
      "metadata": {
        "id": "oITHFK4RhmwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 그래프로 데이터 분포를 파악하기 위해 입출력 데이터를 하나의 테이터 프레임으로 병합\n",
        "concat_data = pd.concat([x_data, y_data], axis=1)\n",
        "display(concat_data)"
      ],
      "metadata": {
        "id": "S-QQ-auL5y2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 목표변수인 당뇨병 진행 상태(Diabetes Progression) 값을 10개의 계급으로 하는 밀도그래프를 출력\n",
        "# 평균적으로 당뇨병 진행 상태(Diabetes Progression) 값은 100에 많이 분포 \n",
        "sns.set(rc={'figure.figsize' : (8, 4)})\n",
        "sns.kdeplot(data=concat_data, x='target', shade=True)\n",
        "plt.xlabel('Diabetes Progression')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ksYAbMsq74Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 변수 간 상관계수를 히트맵 그래프로 출력 \n",
        "# s1 변수와 s2 변수들은 양의 선형적 관계를 가지는 매우 강한 상관관계를 가지고 있음\n",
        "# s3 변수와 s4 변수들은 음의 선형적 관계를 가지는 매우 강한 상관관계를 가지고 있음\n",
        "corr_matrix = concat_data.corr().round(2)\n",
        "\n",
        "sns.set(rc={'figure.figsize' : (8, 4)})\n",
        "sns.heatmap(data=corr_matrix, xticklabels=True, annot=True)\n",
        "plt.xticks(rotation=0)\n",
        "plt.xlabel('\\n< Correlation coefficient between each variable >')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QBWeOEFKhos_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 독립 변수 간 매우 강한 상관관계를 가지는 변수가 있는 경우, 다중공선성(multicollinearity) 문제가 발생함 \n",
        "# 따라서 변수 선택 기법을 사용하여 상관관계가 높은 변수를 제거\n",
        "x_data = x_data.drop(['s2','s3'], axis=1) \n",
        "display(x_data)"
      ],
      "metadata": {
        "id": "2QQhuhkuBybx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. 데이터 분리**"
      ],
      "metadata": {
        "id": "HDuDCFAClikQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용과 테스트용 데이터를 7:3으로 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=20183047)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=20183047)\n",
        "\n",
        "print(f\"- 학습용 입력 데이터(X) 형상 : {x_train.shape}\")\n",
        "print(f\"- 학습용 정답 데이터(Y) 형상 : {y_train.shape}\", end=\"\\n\\n\")\n",
        "print(f\"- 검증용 입력 데이터(X) 형상 : {x_val.shape}\")\n",
        "print(f\"- 검증용 정답 데이터(Y) 형상 : {y_val.shape}\", end=\"\\n\\n\") \n",
        "print(f\"- 평가용 입력 데이터(X) 형상 : {x_test.shape}\")\n",
        "print(f\"- 평가용 정답 데이터(Y) 형상 : {y_test.shape}\")   "
      ],
      "metadata": {
        "id": "izvBl0l_kTSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. 피처 스케일링**"
      ],
      "metadata": {
        "id": "8ceb66rNuKDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최솟값은 0, 최댓값은 1이 되도록 데이터에 대해 정규화\n",
        "# 최소-최대 정규화 스케일러 생성\n",
        "minmax_scalerX = MinMaxScaler()\n",
        "minmax_scalerY = MinMaxScaler()\n",
        "\n",
        "# 정규화 스케일러를 학습용 데이터에 맞춤\n",
        "minmax_scalerX.fit(x_train)\n",
        "minmax_scalerY.fit(y_train)\n",
        "\n",
        "# 정규화 스케일러로 학습 데이터를 변환\n",
        "x_train_minmax = minmax_scalerX.transform(x_train)\n",
        "y_train_minmax = minmax_scalerY.transform(y_train)\n",
        "\n",
        "# 정규화 스케일러로 검증용 데이터를 변환\n",
        "x_val_minmax = minmax_scalerX.transform(x_val)\n",
        "y_val_minmax = minmax_scalerY.transform(y_val)\n",
        "\n",
        "# 정규화 스케일러로 테스트 데이터를 변환\n",
        "x_test_minmax = minmax_scalerX.transform(x_test)\n",
        "y_test_minmax = minmax_scalerY.transform(y_test)"
      ],
      "metadata": {
        "id": "duDnjfGcuD4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. 모형화 및 학습 & 테스트**"
      ],
      "metadata": {
        "id": "S5FzERhrveb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. 입출력 노드 : 6개 / 1개\n",
        "   - 학습 시에 입력 변수의 특성 갯수가 8개이고, 목표 변수 갯수가 1개이기 때문에, 그에 대응하는 입출력 노드로 구성\n",
        "\n",
        "2. 은닉층 개수 (노드 수) : 3개 (60, 120, 60)\n",
        "    - 총 3개의 은닉층이 존재하며, 제 1 은닉층과 제 3 은닉층은 6개의 노드가 존재하고 제 2 은닉층에는 12개의 노드가 존재\n",
        "\n",
        "3. 배치 정규화\n",
        "    - 각 층(layer)을 거칠 때마다 입력 데이터의 분포가 변화함에 따라 학습이 불안정해지는 문제인 내부 공변량(internal covariate shift)를 막기 위해 사용\n",
        "    - 각 층에서 입력 데이터를 정규화하고, 학습 중에 이에 대한 평균과 분산을 조절하여 입력 데이터의 분포를 안정화 가능\n",
        "\n",
        "4. 활성화 함수 :  Relu\n",
        "   - 입력값이 0보다 작을 경우는 0으로 출력하고, 0보다 큰 경우는 그대로 출력하는 비선형 함수인 Relu 함수로 설정\n",
        "   - ReLU 활성화 함수를 사용할 때, 가중치 초기화에 따른 그래디언트 소실 문제를 완화하기 위해 은닉층의 가중치는 He 초깃값을 사용\n",
        "\n",
        "5. 최적화 알고리즘 \n",
        "   - Momentum과 RMSProp의 장점을 결합한 최적화 알고리즘인 Adam(Adaptive Moment Estimation)을 사용\n",
        "   - Momentum은 : 기울기의 방향을 고려하여 학습 속도를 조절 \n",
        "   - RMSProp : 기울기 크기를 고려하여 학습 속도를 조절\n",
        "\n",
        "6. 손실 함수 : \n",
        "   - 예측값과 실제값의 차이를 제곱한 값의 평균을 계산함으로써, \n",
        "     예측값과 실제값 사이의 오차를 잘 나타내는 MSE(Mean Squared Error)를 사용\n",
        "\n",
        "7. 정확도 평가 지표\n",
        "   - 예측값과 실제값의 백분율 차이의 절대값을 평균하는 MAPE(Mean Absolute Percentage Error)를 사용\n",
        "     회귀분석에서 가장 일반적으로 사용되는 평가지표 중 상대적인 오차의 크기를 평가하므로, \n",
        "     이 평가지표의 오차 값은 예측값과 실제값이 클수록 더 커지는 경향이 있음\n",
        "\n",
        "8. 배치 사이즈 / 최대 학습 반복 횟수 : 64 / 1000\n",
        "\"\"\"\n",
        "\n",
        "# 모형 구조\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(60, input_dim=6, activation='relu', kernel_initializer=initializers.HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(120, activation='relu', kernel_initializer=initializers.HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(60, activation='relu', kernel_initializer=initializers.HeNormal()))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, bias_initializer=initializers.Constant(value=0.01)))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.0001), loss='mse')\n",
        "\n",
        "results_standard = model.fit(x_train_minmax, y_train_minmax, validation_data=(x_val_minmax, y_val_minmax)\n",
        "            , epochs=1000, batch_size=64)"
      ],
      "metadata": {
        "id": "X8GSqi2KvNGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAPE 값 출력 \n",
        "y_pred = model.predict(x_test_minmax)\n",
        "y_pred_inverse = minmax_scalerY.inverse_transform(y_pred)\n",
        "\n",
        "minmax_mape = mean_absolute_percentage_error(y_test, y_pred_inverse)\n",
        "print(\"MAPE based on min-max normalization : {:.2%}\".format(minmax_mape))"
      ],
      "metadata": {
        "id": "HYfANNIwiGTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss 그래프 출력\n",
        "train_loss = results_standard.history['loss']\n",
        "val_loss = results_standard.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0,4])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FbgMvHqICLtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측값 대비 실게값의 산포도\n",
        "y_pred = model.predict(x_test_minmax)\n",
        "diff = np.abs(y_pred - y_test_minmax)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.scatter(y_test_minmax, y_pred, c=diff, cmap='viridis')\n",
        "plt.plot([0, 1], [0, 1], c='r')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YKye9tlRCv2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}