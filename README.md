# ğŸ¥  Performance Optimization of MLP for Regression Problem  
<br/>
  
### 1. &nbsp; Research Objective <br/><br/>

- _This study aims to train and evaluate a multilayer perceptron model on the Diabetes 130-US hospitals for years 1999-2008 dataset. To design the optimal model, I referred to the method proposed in the paper "Diabetes Mellitus Diagnosis Using Artificial Neural Networks with Fewer Features". In this paper, the authors tested the number of hidden layers from 2 to 8 to optimize the number of hidden layers for the Pima Indians Diabetes Database dataset and found that 3 or 4 hidden layers were the optimal model structure._  <br/>

- _Since the dataset used in this study is similar to the dataset used in the paper, I limited the network structure to three hidden layers based on the experiments in the paper. However, it is not completely identical to the network structure presented in the paper.In this study, I increased the number of nodes in each layer by about 10 times than the number of nodes presented in the paper to increase the complexity of the model. In addition, I added batch regularization and dropout layers to solve internal covariate shift and overfitting problems. Furthermore, I conducted experiments based on various trial and error methods, such as variable selection techniques, scaling techniques, and initialization techniques, to solve multicollinearity problems and improve the model's performance. Through these efforts, I hope to improve performance of the model and prevented problems such as gradient vanishing._ <br/><br/><br/> 

### 2. &nbsp; Key Components of the Neural Network Model and Experimental Settings  <br/><br/>

- _Input Nodes & Output Nodes_<br/>

  - _As the model has 6 input variables and 1 target variable, the input/output nodes were configured accordingly._ <br/>
  
  - _For reference, the diabetes dataset (Diabetes 130-US hospitals for years 1999-2008 Data Set) has 10 characteristics, but I removed the characteristics of age and sex from the input variables because I thought they were variables to be excluded from the analysis._ <br/>
  
  - _In addition, to avoid multicollinearity issues due to highly correlated independent variables, I used a variable selection technique to remove two highly correlated variables._ <br/><br/>

- _Hidden Layers & Batch Normalization & Dropout_<br/>

  - _There are a total of 3 hidden layers, with 60 nodes in the 1st and 3rd hidden layers, and 120 nodes in the 2nd hidden layer._<br/>
  
  - _To prevent the internal covariate shift problem where the distribution of input data changes with each hidden layer, I used batch normalization layers._ <br/>
  
  - _In addition, I used dropout layers, which randomly drop out some neurons during the learning process, to prevent overfitting and improve the generalization performance of the model._ <br/><br/>

- _Activation Function: ReLU (Rectified Linear Unit)_ <br/>

  - _I set the activation function to the ReLU function, which is a non-linear function that outputs 0 when the input value is less than 0, and outputs the input value itself when it is greater than 0._

  - _Also, depending on how you initialize the hidden layer weights when using the ReLU activation function, the gradient may disappear during the model training process. To avoid this problem, I used the He initialization technique, which is a neural network initialization method that takes nonlinearity into account like the ReLU activation function._<br/><br/>
  
- _Optimization Algorithm : Adam (Adaptive Moment Estimation)_ <br/>

  - _I used the Adam (Adaptive Moment Estimation) optimization algorithm, which combines the advantages of Momentum, which adjusts the learning rate considering the direction of the gradient, and RMSProp, which adjusts the learning rate considering the magnitude of the gradient._<br/><br/>

- _Cost Function : MSE (Mean Squared Error)_ <br/>

  - _I used the mean square error (MSE), which calculates the average of the squared differences between the predicted and actual values._<br/>

  - _MSE has the advantage of being easy to converge to the optimal value because the shift changes differently as you get closer to the optimal value, but it also has the disadvantage of being highly influenced by outliers._ <br/><br/>

- _Evaluation Metric: MAPE (Mean Absolute Percentage Error)_ <br/>

  - _I used MAPE (Mean Absolute Percentage Error), which calculates the mean absolute percentage difference between predicted values and actual values._<br/>

  - _MAPE is one of the most commonly used metrics in regression analysis and is characterized by the fact that the larger the difference between predicted and actual values, the larger the value of MAPE. In other words, the better the model predicts, the smaller the value of MAPE will be._ <br/><br/>


- _Maximum Number of Learning Iterations_ <br/>

  - _In this experiment, the model is trained by iterating up to a maximum of 1000 times._<br/>
  
  - _The number of iterations during training affects the speed and accuracy of the model, and I, as the researcher conducting the experiment, have set the number of iterations based on my experience of tuning deep learning models._<br/>
  
  - _Setting the number of iterations too low can result in underfitting, where the model is not trained enough, while setting it too high can result in overfitting._<br/> 
  
  - _Therefore, I have chosen what I believe to be the most appropriate number of iterations._ <br/> <br/> <br/> 

### 3. &nbsp; Data Preprocessing and Analysis <br/><br/>

- _**Package Settings**_ <br/> 
  
  ```
  from sklearn.datasets import load_diabetes
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import MinMaxScaler, StandardScaler
  from sklearn.metrics import mean_absolute_percentage_error

  from keras import initializers
  from keras.optimizers import Adam
  from keras.models import Sequential
  from keras.layers import Dense, Dropout,BatchNormalization

  import numpy as np
  import pandas as pd
  import seaborn as sns
  import matplotlib.pyplot as plt
  from IPython.display import display
  ```

- _**Data Preparation**_ <br/> 
  
  ```
  # ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ ë¡œë”© : ì…ë ¥ ë°ì´í„°(data), ëª©í‘œ ë°ì´í„°(target)
  diabetes = load_diabetes()

  # ì…ë ¥ ë°ì´í„°ì™€ ëª©í‘œ ë°ì´í„°ë¥¼ ê°ê° ë°ì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
  x_data = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
  y_data = pd.DataFrame(diabetes.target, columns=['target'])

  # ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ì— NaNê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
  if x_data.isnull().values.any() or y_data.isnull().values.any():
      print("- ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ì—ëŠ” NaNê°’ì´ ì¡´ì¬í•©ë‹ˆë‹¤. -", end= "\n\n")
  else:
      print("- ë‹¹ë‡¨ë³‘ ë°ì´í„° ì…‹íŠ¸ì—ëŠ” NaNê°’ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. -", end= "\n\n")
  ```
  
  ```
  # ì…ë ¥ ë°ì´í„°ë¥¼ ì¶œë ¥
  print(f"< ì…ë ¥ ë°ì´í„°ì˜ êµ¬ì„± : {x_data.shape[0]}í–‰ x {x_data.shape[1]}ì—´ >")
  display(x_data)
  ```
  
  ```
  # ë¶„ì„ ëŒ€ìƒì—ì„œ ì œì™¸í•  ë³€ìˆ˜ì¸ "age & sex" ì—´ì„ ì‚­ì œ  
  x_data = x_data.drop(['age', 'sex'], axis=1) 

  #  ë¶„ì„ ëŒ€ìƒì—ì„œ ì œì™¸í•  ë³€ìˆ˜ì¸ "age & sex" ì—´ì„ ì‚­ì œí•œ ì…ë ¥ ë°ì´í„°ë¥¼ ì¶œë ¥
  print(f"\n< 'age & sex' ì—´ì„ ì‚­ì œí•œ ì…ë ¥ ë°ì´í„°ì˜ êµ¬ì„± : {x_data.shape[0]}í–‰ x {x_data.shape[1]}ì—´ >")
  display(x_data)
  ```
  
  ```
  # ëª©í‘œ ë°ì´í„°ë¥¼ ì¶œë ¥
  print(f"\n< ëª©í‘œ ë°ì´í„°ì˜ êµ¬ì„± : {y_data.shape[0]}í–‰ x {y_data.shape[1]}ì—´ >")
  display(y_data)
  ```
  
- _**Exploratory Data Analysis (EDA)**_ <br/> 
  
  ```
  # ê·¸ë˜í”„ë¡œ ë°ì´í„° ë¶„í¬ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì…ì¶œë ¥ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ í…Œì´í„° í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©
  concat_data = pd.concat([x_data, y_data], axis=1)
  display(concat_data)
  ```

  ```
  # ëª©í‘œë³€ìˆ˜ì¸ ë‹¹ë‡¨ë³‘ ì§„í–‰ ìƒíƒœ(Diabetes Progression) ê°’ì„ 10ê°œì˜ ê³„ê¸‰ìœ¼ë¡œ í•˜ëŠ” ë°€ë„ê·¸ë˜í”„ë¥¼ ì¶œë ¥
  # í‰ê· ì ìœ¼ë¡œ ë‹¹ë‡¨ë³‘ ì§„í–‰ ìƒíƒœ(Diabetes Progression) ê°’ì€ 100ì— ë§ì´ ë¶„í¬ 
  sns.set(rc={'figure.figsize' : (15, 3)})
  sns.kdeplot(data=concat_data, x='target', shade=True)
  plt.xlabel('Diabetes Progression')
  plt.show()
  ```
  
  <img src="https://github.com/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/image/density_graph.png?raw=true" height="320">
  
  ```
  # ê° ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ë¥¼ íˆíŠ¸ë§µ ê·¸ë˜í”„ë¡œ ì¶œë ¥ 
  # s1 ë³€ìˆ˜ì™€ s2 ë³€ìˆ˜ë“¤ì€ ì–‘ì˜ ì„ í˜•ì  ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŒ
  # s3 ë³€ìˆ˜ì™€ s4 ë³€ìˆ˜ë“¤ì€ ìŒì˜ ì„ í˜•ì  ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŒ
  corr_matrix = concat_data.corr().round(2)

  sns.set(rc={'figure.figsize' : (8, 5)})
  sns.heatmap(data=corr_matrix, xticklabels=True, annot=True)
  plt.xticks(rotation=0)
  plt.xlabel('\n< Correlation coefficient between each variable >')
  plt.show()
  ```
  
  <img src="https://github.com/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/image/corr_heatmap_graph.png?raw=true" width="640">
  
  ```
  # ë…ë¦½ ë³€ìˆ˜ ê°„ ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°, ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) ë¬¸ì œê°€ ë°œìƒí•¨ 
  # ë”°ë¼ì„œ ë³€ìˆ˜ ì„ íƒ ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ë¥¼ ì œê±°
  x_data = x_data.drop(['s2','s3'], axis=1) 
  display(x_data)
  ``` 
  
- _**Splitting Data**_ <br/>  
  
  ```
  # í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¥¼ 7:3ìœ¼ë¡œ ë¶„ë¦¬
  x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=20183047)
  x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=20183047)

  print(f"- í•™ìŠµìš© ì…ë ¥ ë°ì´í„°(X) í˜•ìƒ : {x_train.shape}")
  print(f"- í•™ìŠµìš© ì •ë‹µ ë°ì´í„°(Y) í˜•ìƒ : {y_train.shape}", end="\n\n")
  print(f"- ê²€ì¦ìš© ì…ë ¥ ë°ì´í„°(X) í˜•ìƒ : {x_val.shape}")
  print(f"- ê²€ì¦ìš© ì •ë‹µ ë°ì´í„°(Y) í˜•ìƒ : {y_val.shape}", end="\n\n") 
  print(f"- í‰ê°€ìš© ì…ë ¥ ë°ì´í„°(X) í˜•ìƒ : {x_test.shape}")
  print(f"- í‰ê°€ìš© ì •ë‹µ ë°ì´í„°(Y) í˜•ìƒ : {y_test.shape}"))   
  ```
  
- _**Feature Scaling**_ <br/> 
  
  ```
  # ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ì´ ë˜ë„ë¡ ë°ì´í„°ì— ëŒ€í•´ ì •ê·œí™”
  # ìµœì†Œ-ìµœëŒ€ ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
  minmax_scalerX = MinMaxScaler()
  minmax_scalerY = MinMaxScaler()

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ í•™ìŠµìš© ë°ì´í„°ì— ë§ì¶¤
  minmax_scalerX.fit(x_train)
  minmax_scalerY.fit(y_train)

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ë³€í™˜
  x_train_minmax = minmax_scalerX.transform(x_train)
  y_train_minmax = minmax_scalerY.transform(y_train)

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¡œ ê²€ì¦ìš© ë°ì´í„°ë¥¼ ë³€í™˜
  x_val_minmax = minmax_scalerX.transform(x_val)
  y_val_minmax = minmax_scalerY.transform(y_val)

  # ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë³€í™˜
  x_test_minmax = minmax_scalerX.transform(x_test)
  y_test_minmax = minmax_scalerY.transform(y_test)
  ```
  
  ```
  # ìµœì†Ÿê°’ì€ 0, ìµœëŒ“ê°’ì€ 1ì´ ë˜ë„ë¡ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ì •ê·œí™”
  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í•™ìŠµ ë°ì´í„°ì˜ ì…ë ¥ ê°’
  scalerX = MinMaxScaler()
  scalerX.fit(x_train)
  x_train_norm = scalerX.transform(x_train)

  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í•™ìŠµ ë°ì´í„°ì˜ ì¶œë ¥ ê°’
  scalerY = MinMaxScaler()
  scalerY.fit(y_train)
  y_train_norm = scalerY.transform(y_train)

  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ì…ì¶œë ¥ ê°’
  x_test_norm = scalerX.transform(x_test)
  y_test_norm = scalerY.transform(y_test)
  ```
  <br/> 

### 4. &nbsp; Training and Testing MLP Model <br/><br/>

- _Optimized MLP Model_

  ```
  """
  1. ì…ì¶œë ¥ ë…¸ë“œ : 6ê°œ / 1ê°œ
     - í•™ìŠµ ì‹œì— ì…ë ¥ ë³€ìˆ˜ì˜ íŠ¹ì„± ê°¯ìˆ˜ê°€ 8ê°œì´ê³ , ëª©í‘œ ë³€ìˆ˜ ê°¯ìˆ˜ê°€ 1ê°œì´ê¸° ë•Œë¬¸ì—, ê·¸ì— ëŒ€ì‘í•˜ëŠ” ì…ì¶œë ¥ ë…¸ë“œë¡œ êµ¬ì„±

  2. ì€ë‹‰ì¸µ ê°œìˆ˜ (ë…¸ë“œ ìˆ˜) : 3ê°œ (60, 120, 60)
      - ì´ 3ê°œì˜ ì€ë‹‰ì¸µì´ ì¡´ì¬í•˜ë©°, ì œ 1 ì€ë‹‰ì¸µê³¼ ì œ 3 ì€ë‹‰ì¸µì€ 6ê°œì˜ ë…¸ë“œê°€ ì¡´ì¬í•˜ê³  ì œ 2 ì€ë‹‰ì¸µì—ëŠ” 12ê°œì˜ ë…¸ë“œê°€ ì¡´ì¬

  3. ë°°ì¹˜ ì •ê·œí™”
      - ê° ì¸µ(layer)ì„ ê±°ì¹  ë•Œë§ˆë‹¤ ì…ë ¥ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë³€í™”í•¨ì— ë”°ë¼ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ëŠ” ë¬¸ì œì¸ ë‚´ë¶€ ê³µë³€ëŸ‰(internal covariate shift)ë¥¼ ë§‰ê¸° ìœ„í•´ ì‚¬ìš©
      - ê° ì¸µì—ì„œ ì…ë ¥ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³ , í•™ìŠµ ì¤‘ì— ì´ì— ëŒ€í•œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¡°ì ˆí•˜ì—¬ ì…ë ¥ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì•ˆì •í™” ê°€ëŠ¥

  4. í™œì„±í™” í•¨ìˆ˜ :  Relu
     - ì…ë ¥ê°’ì´ 0ë³´ë‹¤ ì‘ì„ ê²½ìš°ëŠ” 0ìœ¼ë¡œ ì¶œë ¥í•˜ê³ , 0ë³´ë‹¤ í° ê²½ìš°ëŠ” ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì¸ Relu í•¨ìˆ˜ë¡œ ì„¤ì •
     - ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ, ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ë”°ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™”í•˜ê¸° ìœ„í•´ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ëŠ” He ì´ˆê¹ƒê°’ì„ ì‚¬ìš©

  5. ìµœì í™” ì•Œê³ ë¦¬ì¦˜ 
     - Momentumê³¼ RMSPropì˜ ì¥ì ì„ ê²°í•©í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì¸ Adam(Adaptive Moment Estimation)ì„ ì‚¬ìš©
     - Momentumì€ : ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì„ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ 
     - RMSProp : ê¸°ìš¸ê¸° í¬ê¸°ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆ

  6. ì†ì‹¤ í•¨ìˆ˜ : 
     - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•œ ê°’ì˜ í‰ê· ì„ ê³„ì‚°í•¨ìœ¼ë¡œì¨, 
       ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ì˜ ë‚˜íƒ€ë‚´ëŠ” MSE(Mean Squared Error)ë¥¼ ì‚¬ìš©

  7. ì •í™•ë„ í‰ê°€ ì§€í‘œ
     - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ë°±ë¶„ìœ¨ ì°¨ì´ì˜ ì ˆëŒ€ê°’ì„ í‰ê· í•˜ëŠ” MAPE(Mean Absolute Percentage Error)ë¥¼ ì‚¬ìš©
       íšŒê·€ë¶„ì„ì—ì„œ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” í‰ê°€ì§€í‘œ ì¤‘ ìƒëŒ€ì ì¸ ì˜¤ì°¨ì˜ í¬ê¸°ë¥¼ í‰ê°€í•˜ë¯€ë¡œ, 
       ì´ í‰ê°€ì§€í‘œì˜ ì˜¤ì°¨ ê°’ì€ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì´ í´ìˆ˜ë¡ ë” ì»¤ì§€ëŠ” ê²½í–¥ì´ ìˆìŒ

  8. ë°°ì¹˜ ì‚¬ì´ì¦ˆ / ìµœëŒ€ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ : 64 / 1000
  """

  # ëª¨í˜• êµ¬ì¡°
  model = Sequential()

  model.add(Dense(60, input_dim=6, activation='relu', kernel_initializer=initializers.HeNormal()))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(120, activation='relu', kernel_initializer=initializers.HeNormal()))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(60, activation='relu', kernel_initializer=initializers.HeNormal()))
  model.add(BatchNormalization())
  model.add(Dropout(0.5))

  model.add(Dense(1, bias_initializer=initializers.Constant(value=0.01)))

  model.compile(optimizer=Adam(lr=0.0001), loss='mse')

  results_standard = model.fit(x_train_minmax, y_train_minmax, validation_data=(x_val_minmax, y_val_minmax)
              , epochs=1000, batch_size=64)
  ```

  ```
  # MAPE ê°’ ì¶œë ¥ 
  y_pred = model.predict(x_test_minmax)
  y_pred_inverse = minmax_scalerY.inverse_transform(y_pred)

  minmax_mape = mean_absolute_percentage_error(y_test, y_pred_inverse)
  print("MAPE based on min-max normalization : {:.2%}".format(minmax_mape))
  ```
  <br/> 
  
### 5. &nbsp; Research Results  <br/><br/>
    
- _The purpose of this study was to train and evaluate a multilayer perceptron model on the Diabetes 130-US hospitals for years 1999-2008 Data Set. I set the number of hidden racers by referring to the optimal model structure proposed in the paper "Diabetes Mellitus Diagnosis Using Artificial Neural Networks with Fewer Features", increased the model complexity by making the number of nodes in each layer about 10 times larger than the number of nodes set in the paper, and added batch regularization and dropout layers to solve the problems of internal covariate shift and overfitting. In addition, I improved the performance of the existing model by using variable selection techniques, scaling techniques, and initialization techniques to solve multicollinearity problems and to prevent problems such as gradients vanishing._ <br/> <br/>

  ```
  # loss ê·¸ë˜í”„ ì¶œë ¥
  train_loss = results_standard.history['loss']
  val_loss = results_standard.history['val_loss']

  epochs = range(1, len(train_loss) + 1)

  plt.plot(epochs, train_loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'r', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.ylim([0,4])
  plt.legend()
  plt.show()
  ```
  
  ```
  # ì˜ˆì¸¡ê°’ ëŒ€ë¹„ ì‹¤ê²Œê°’ì˜ ì‚°í¬ë„
  y_pred = model.predict(x_test_minmax)
  diff = np.abs(y_pred - y_test_minmax)

  plt.figure(figsize=(5, 5))
  plt.scatter(y_test_minmax, y_pred, c=diff, cmap='viridis')
  plt.plot([0, 1], [0, 1], c='r')
  plt.xlabel('True Values')
  plt.ylabel('Predictions')
  plt.colorbar()
  plt.show()
  ```
  <br/>
  <img src="https://github.com/qortmdgh4141/Performance-Optimization-of-MLP-Model-for-Regression-Problem/blob/main/image/scatter_plot_line_graph.png?raw=true" weight="1280">

- _The following graph shows the evolution of the loss with increasing epoch, and I can see that overfitting did not occur due to the impact of the aforementioned batch regularization and dropout layer._ <br/>

- _However, it was found that underfitting occurs, where the predictive performance on the training data does not improve because it does not sufficiently reflect the complexity of the data after some learning progress._ <br/>

- _This means that the model has been oversimplified, and I believe that the following reasons contributed to this:_ <br/>

  - _Low Model Complexity_<br/>
  
    - _I believe that underfitting occurs because the model is too simple or limited._<br/>
    
    - _This does not mean that making the current model structure more complex is a good solution. This is because the amount of training data is currently small, and making the model structure more complex is very likely to lead to overfitting._ <br/><br/>
    
  - _Lack of Variable Diversity_<br/>
  
    - _It is determined that underfitting occurred due to a lack of variable diversity in the dataset._<br/>
    
    - _To be more specific, I believe that excessive normalization was applied to a dataset that lacks diversity, causing the model to fail to adequately reflect the patterns in the training data._<br/> <br/> <br/>

### 6. &nbsp; Suggestions for Future Research  <br/><br/>
    
- _Based on the scatter plot of predicted values compared to actual values, it can be seen that the performance of the prediction model is not very accurate. Therefore, to improve the performance of the regression model in the future, the following solutions can be suggested:_

  - _Diversity and Quality of Data_<br/>
  
    - _The unbalanced distribution of data can greatly affect the performance of the model.Therefore, if the data is collected considering the diversity of the data and preprocessed so as not to harm the diversity of the collected data set, the performance of the model will be improved._<br/>

  - _Feature Engineering_<br/>
  
    - _Feature engineering is known to have a significant impact on the performance of a prediction model.Therefore, by extracting more diverse and sophisticated features or introducing new variables, it is expected to improve the performance of the model._<br/> <br/> <br/>
 
--------------------------
### ğŸ’» S/W Development Environment
<p>
  <img src="https://img.shields.io/badge/Windows 10-0078D6?style=flat-square&logo=Windows&logoColor=white"/>
  <img src="https://img.shields.io/badge/Google Colab-black?style=flat-square&logo=Google Colab&logoColor=yellow"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
</p>
<p>
  <img src="https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=keras&logoColor=white"/>
  <img src="https://img.shields.io/badge/scikit learn-blue?style=flat-square&logo=scikitlearn&logoColor=F7931E"/>
  <img src="https://img.shields.io/badge/Numpy-013243?style=flat-square&logo=Numpy&logoColor=blue"/>
</p>

### ğŸš€ Machine Learning Model
<p>
  <img src="https://img.shields.io/badge/MLP-5C5543?style=flat-square?"/>
</p> 

### ğŸ’¾ Datasets used in the project
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Diabetes Dataset <br/>
